## Tokenizer training
timestamp: 2026-01-25 01:05:51

- max_chars: 10,000,000,000
- doc_cap: 10,000
- vocab_size: 32,768
- train_time: 330.7618
- num_special_tokens: 9
- token_bytes_min: 1
- token_bytes_max: 19
- token_bytes_mean: 6.5998
- token_bytes_std: 2.8246

