Using Master Port: 27673
Autodetected device type: cuda
Target examples per step: 32
Device batch size: 4
Examples per step is device_batch_size * ddp_world_size: 8
=> Setting grad accum steps: 4
Scaling the LR for the AdamW parameters ∝1/√(1664/768) = 0.679366
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([32768, 1664]), sliced=True
AdamW: 1 param of shape torch.Size([26]), sliced=False
AdamW: 1 param of shape torch.Size([26]), sliced=False
Muon: 13 params of shape torch.Size([13, 32]), chunk_size=7
Muon: 104 params of shape torch.Size([1664, 1664]), chunk_size=52
Muon: 26 params of shape torch.Size([1664, 6656]), chunk_size=13
Muon: 26 params of shape torch.Size([6656, 1664]), chunk_size=13
Step 00000 | Validation loss: 0.791633
Step 00000/00701 | Training loss: 0.460109| lrm: 1.000000| num_tokens: 9,256
Step 00001/00701 | Training loss: 0.743806| lrm: 0.998573| num_tokens: 12,418
Step 00002/00701 | Training loss: 0.154697| lrm: 0.997147| num_tokens: 9,005
Step 00003/00701 | Training loss: 0.473679| lrm: 0.995720| num_tokens: 15,020
Step 00004/00701 | Training loss: 0.531220| lrm: 0.994294| num_tokens: 8,762
Step 00005/00701 | Training loss: 0.557092| lrm: 0.992867| num_tokens: 13,129
Step 00006/00701 | Training loss: 0.701576| lrm: 0.991441| num_tokens: 11,899
Step 00007/00701 | Training loss: 0.651344| lrm: 0.990014| num_tokens: 9,923
Step 00008/00701 | Training loss: 0.190652| lrm: 0.988588| num_tokens: 11,216
Step 00009/00701 | Training loss: 0.580322| lrm: 0.987161| num_tokens: 9,896
Step 00010/00701 | Training loss: 0.706661| lrm: 0.985735| num_tokens: 9,564
Step 00011/00701 | Training loss: 0.651955| lrm: 0.984308| num_tokens: 13,669
Step 00012/00701 | Training loss: 0.367010| lrm: 0.982882| num_tokens: 13,885
Step 00013/00701 | Training loss: 0.491027| lrm: 0.981455| num_tokens: 6,030
Step 00014/00701 | Training loss: 0.345524| lrm: 0.980029| num_tokens: 11,619
Step 00015/00701 | Training loss: 0.504179| lrm: 0.978602| num_tokens: 12,139
Step 00016/00701 | Training loss: 0.934047| lrm: 0.977175| num_tokens: 8,654
Step 00017/00701 | Training loss: 0.807827| lrm: 0.975749| num_tokens: 10,094
Step 00018/00701 | Training loss: 0.667339| lrm: 0.974322| num_tokens: 11,479
Step 00019/00701 | Training loss: 0.903045| lrm: 0.972896| num_tokens: 12,562
Step 00020/00701 | Training loss: 0.503465| lrm: 0.971469| num_tokens: 9,883
Step 00021/00701 | Training loss: 0.656198| lrm: 0.970043| num_tokens: 14,229
Step 00022/00701 | Training loss: 0.618050| lrm: 0.968616| num_tokens: 10,116
Step 00023/00701 | Training loss: 0.527886| lrm: 0.967190| num_tokens: 11,300
Step 00024/00701 | Training loss: 0.358523| lrm: 0.965763| num_tokens: 8,767
Step 00025/00701 | Training loss: 0.757258| lrm: 0.964337| num_tokens: 13,852
Step 00026/00701 | Training loss: 0.608028| lrm: 0.962910| num_tokens: 13,584
Step 00027/00701 | Training loss: 0.575204| lrm: 0.961484| num_tokens: 15,811
Step 00028/00701 | Training loss: 0.537229| lrm: 0.960057| num_tokens: 11,711
Step 00029/00701 | Training loss: 0.477803| lrm: 0.958631| num_tokens: 15,560
Step 00030/00701 | Training loss: 0.711810| lrm: 0.957204| num_tokens: 13,706
Step 00031/00701 | Training loss: 0.587943| lrm: 0.955777| num_tokens: 10,480
Step 00032/00701 | Training loss: 0.398108| lrm: 0.954351| num_tokens: 10,996
Step 00033/00701 | Training loss: 0.571669| lrm: 0.952924| num_tokens: 12,816
Step 00034/00701 | Training loss: 0.599302| lrm: 0.951498| num_tokens: 14,202
Step 00035/00701 | Training loss: 0.754831| lrm: 0.950071| num_tokens: 9,657
Step 00036/00701 | Training loss: 0.591208| lrm: 0.948645| num_tokens: 8,998
Step 00037/00701 | Training loss: 0.670005| lrm: 0.947218| num_tokens: 15,912
Step 00038/00701 | Training loss: 0.374031| lrm: 0.945792| num_tokens: 12,802
Step 00039/00701 | Training loss: 0.428236| lrm: 0.944365| num_tokens: 10,686
Step 00040/00701 | Training loss: 0.664154| lrm: 0.942939| num_tokens: 10,745
Step 00041/00701 | Training loss: 0.463161| lrm: 0.941512| num_tokens: 9,486
Step 00042/00701 | Training loss: 0.650938| lrm: 0.940086| num_tokens: 12,807
Step 00043/00701 | Training loss: 0.795009| lrm: 0.938659| num_tokens: 14,955
Step 00044/00701 | Training loss: 0.596811| lrm: 0.937233| num_tokens: 12,047
Step 00045/00701 | Training loss: 0.310643| lrm: 0.935806| num_tokens: 10,546
Step 00046/00701 | Training loss: 0.337864| lrm: 0.934379| num_tokens: 11,886
Step 00047/00701 | Training loss: 0.415276| lrm: 0.932953| num_tokens: 12,793
Step 00048/00701 | Training loss: 0.617313| lrm: 0.931526| num_tokens: 7,893
Step 00049/00701 | Training loss: 0.723880| lrm: 0.930100| num_tokens: 11,934
Step 00050/00701 | Training loss: 0.450865| lrm: 0.928673| num_tokens: 7,943
Step 00051/00701 | Training loss: 0.516308| lrm: 0.927247| num_tokens: 10,088
Step 00052/00701 | Training loss: 0.463881| lrm: 0.925820| num_tokens: 8,329
Step 00053/00701 | Training loss: 0.607072| lrm: 0.924394| num_tokens: 9,957
Step 00054/00701 | Training loss: 0.505993| lrm: 0.922967| num_tokens: 10,858
Step 00055/00701 | Training loss: 0.554975| lrm: 0.921541| num_tokens: 10,128
Step 00056/00701 | Training loss: 0.661442| lrm: 0.920114| num_tokens: 9,511
Step 00057/00701 | Training loss: 0.681401| lrm: 0.918688| num_tokens: 16,249
Step 00058/00701 | Training loss: 0.631156| lrm: 0.917261| num_tokens: 11,180
Step 00059/00701 | Training loss: 0.763737| lrm: 0.915835| num_tokens: 14,785
Step 00060/00701 | Training loss: 0.942640| lrm: 0.914408| num_tokens: 16,167
Step 00061/00701 | Training loss: 0.552901| lrm: 0.912981| num_tokens: 9,678
Step 00062/00701 | Training loss: 0.635645| lrm: 0.911555| num_tokens: 13,330
Step 00063/00701 | Training loss: 0.588952| lrm: 0.910128| num_tokens: 8,720
Step 00064/00701 | Training loss: 0.538967| lrm: 0.908702| num_tokens: 7,542
Step 00065/00701 | Training loss: 0.664752| lrm: 0.907275| num_tokens: 15,292
Step 00066/00701 | Training loss: 0.660407| lrm: 0.905849| num_tokens: 12,147
Step 00067/00701 | Training loss: 0.646100| lrm: 0.904422| num_tokens: 12,552
Step 00068/00701 | Training loss: 0.677818| lrm: 0.902996| num_tokens: 9,038
Step 00069/00701 | Training loss: 0.478469| lrm: 0.901569| num_tokens: 11,074
Step 00070/00701 | Training loss: 0.683527| lrm: 0.900143| num_tokens: 15,362
Step 00071/00701 | Training loss: 0.755157| lrm: 0.898716| num_tokens: 12,418
Step 00072/00701 | Training loss: 0.544987| lrm: 0.897290| num_tokens: 9,816
Step 00073/00701 | Training loss: 0.975327| lrm: 0.895863| num_tokens: 13,411
Step 00074/00701 | Training loss: 0.544598| lrm: 0.894437| num_tokens: 11,219
Step 00075/00701 | Training loss: 0.598748| lrm: 0.893010| num_tokens: 7,933
Step 00076/00701 | Training loss: 0.599929| lrm: 0.891583| num_tokens: 11,508
Step 00077/00701 | Training loss: 0.597624| lrm: 0.890157| num_tokens: 13,285
Step 00078/00701 | Training loss: 0.521577| lrm: 0.888730| num_tokens: 10,327
Step 00079/00701 | Training loss: 0.550382| lrm: 0.887304| num_tokens: 10,498
Step 00080/00701 | Training loss: 0.195887| lrm: 0.885877| num_tokens: 9,386
Step 00081/00701 | Training loss: 0.907127| lrm: 0.884451| num_tokens: 10,398
Step 00082/00701 | Training loss: 0.924071| lrm: 0.883024| num_tokens: 10,526
Step 00083/00701 | Training loss: 0.458060| lrm: 0.881598| num_tokens: 12,774
Step 00084/00701 | Training loss: 0.558706| lrm: 0.880171| num_tokens: 10,756
Step 00085/00701 | Training loss: 0.690020| lrm: 0.878745| num_tokens: 10,655
Step 00086/00701 | Training loss: 0.366911| lrm: 0.877318| num_tokens: 10,975
Step 00087/00701 | Training loss: 0.334389| lrm: 0.875892| num_tokens: 9,434
Step 00088/00701 | Training loss: 0.603571| lrm: 0.874465| num_tokens: 11,565
Step 00089/00701 | Training loss: 0.518714| lrm: 0.873039| num_tokens: 10,104
Step 00090/00701 | Training loss: 0.739406| lrm: 0.871612| num_tokens: 12,786
Step 00091/00701 | Training loss: 0.444607| lrm: 0.870185| num_tokens: 5,994
Step 00092/00701 | Training loss: 0.806373| lrm: 0.868759| num_tokens: 12,771
Step 00093/00701 | Training loss: 0.584874| lrm: 0.867332| num_tokens: 12,633
Step 00094/00701 | Training loss: 0.596285| lrm: 0.865906| num_tokens: 15,651
Step 00095/00701 | Training loss: 0.839848| lrm: 0.864479| num_tokens: 8,968
Step 00096/00701 | Training loss: 0.359186| lrm: 0.863053| num_tokens: 8,328
Step 00097/00701 | Training loss: 0.505589| lrm: 0.861626| num_tokens: 8,984
Step 00098/00701 | Training loss: 0.575421| lrm: 0.860200| num_tokens: 11,510
Step 00099/00701 | Training loss: 0.701484| lrm: 0.858773| num_tokens: 13,996
Step 00100 | Validation loss: 0.800091
Step 00100/00701 | Training loss: 0.617459| lrm: 0.857347| num_tokens: 13,424
Step 00101/00701 | Training loss: 0.472199| lrm: 0.855920| num_tokens: 12,023
Step 00102/00701 | Training loss: 1.029527| lrm: 0.854494| num_tokens: 12,892
Step 00103/00701 | Training loss: 0.442510| lrm: 0.853067| num_tokens: 11,807
Step 00104/00701 | Training loss: 0.851953| lrm: 0.851641| num_tokens: 12,360
Step 00105/00701 | Training loss: 0.342290| lrm: 0.850214| num_tokens: 9,011
Step 00106/00701 | Training loss: 0.524410| lrm: 0.848787| num_tokens: 9,140
Step 00107/00701 | Training loss: 0.784441| lrm: 0.847361| num_tokens: 11,014
Step 00108/00701 | Training loss: 0.800594| lrm: 0.845934| num_tokens: 12,334
Step 00109/00701 | Training loss: 0.401096| lrm: 0.844508| num_tokens: 11,003
Step 00110/00701 | Training loss: 0.580863| lrm: 0.843081| num_tokens: 7,784
Step 00111/00701 | Training loss: 0.444603| lrm: 0.841655| num_tokens: 13,697
Step 00112/00701 | Training loss: 0.608597| lrm: 0.840228| num_tokens: 12,853
Step 00113/00701 | Training loss: 0.561868| lrm: 0.838802| num_tokens: 14,214
Step 00114/00701 | Training loss: 0.372512| lrm: 0.837375| num_tokens: 12,315
Step 00115/00701 | Training loss: 0.340780| lrm: 0.835949| num_tokens: 10,728
Step 00116/00701 | Training loss: 0.448774| lrm: 0.834522| num_tokens: 8,886
Step 00117/00701 | Training loss: 0.681283| lrm: 0.833096| num_tokens: 11,553
Step 00118/00701 | Training loss: 1.007190| lrm: 0.831669| num_tokens: 8,536
Step 00119/00701 | Training loss: 1.070925| lrm: 0.830243| num_tokens: 8,228
Step 00120/00701 | Training loss: 0.528181| lrm: 0.828816| num_tokens: 9,239
Step 00121/00701 | Training loss: 0.770085| lrm: 0.827389| num_tokens: 17,257
Step 00122/00701 | Training loss: 0.622798| lrm: 0.825963| num_tokens: 10,358
Step 00123/00701 | Training loss: 0.659885| lrm: 0.824536| num_tokens: 6,053
Step 00124/00701 | Training loss: 0.748174| lrm: 0.823110| num_tokens: 14,130
Step 00125/00701 | Training loss: 0.548926| lrm: 0.821683| num_tokens: 12,558
Step 00126/00701 | Training loss: 0.616051| lrm: 0.820257| num_tokens: 11,967
Step 00127/00701 | Training loss: 0.364721| lrm: 0.818830| num_tokens: 12,097
Step 00128/00701 | Training loss: 0.722443| lrm: 0.817404| num_tokens: 10,344
Step 00129/00701 | Training loss: 0.359277| lrm: 0.815977| num_tokens: 11,272
Step 00130/00701 | Training loss: 1.537891| lrm: 0.814551| num_tokens: 9,875
Step 00131/00701 | Training loss: 0.919832| lrm: 0.813124| num_tokens: 11,904
Step 00132/00701 | Training loss: 0.484767| lrm: 0.811698| num_tokens: 11,215
Step 00133/00701 | Training loss: 0.588328| lrm: 0.810271| num_tokens: 14,469
Step 00134/00701 | Training loss: 0.398374| lrm: 0.808845| num_tokens: 10,876
Step 00135/00701 | Training loss: 0.963337| lrm: 0.807418| num_tokens: 12,285
Step 00136/00701 | Training loss: 0.632218| lrm: 0.805991| num_tokens: 6,641
Step 00137/00701 | Training loss: 0.473977| lrm: 0.804565| num_tokens: 9,953
Step 00138/00701 | Training loss: 0.299912| lrm: 0.803138| num_tokens: 8,231
Step 00139/00701 | Training loss: 0.818807| lrm: 0.801712| num_tokens: 8,035
Step 00140/00701 | Training loss: 0.526832| lrm: 0.800285| num_tokens: 15,148
Step 00141/00701 | Training loss: 0.556976| lrm: 0.798859| num_tokens: 15,376
Step 00142/00701 | Training loss: 0.380925| lrm: 0.797432| num_tokens: 10,166
Step 00143/00701 | Training loss: 0.699726| lrm: 0.796006| num_tokens: 9,652
Step 00144/00701 | Training loss: 0.664542| lrm: 0.794579| num_tokens: 10,551
Step 00145/00701 | Training loss: 0.522175| lrm: 0.793153| num_tokens: 10,809
Step 00146/00701 | Training loss: 0.565313| lrm: 0.791726| num_tokens: 10,709
Step 00147/00701 | Training loss: 0.590101| lrm: 0.790300| num_tokens: 11,856
Step 00148/00701 | Training loss: 0.573475| lrm: 0.788873| num_tokens: 12,369
Step 00149/00701 | Training loss: 0.568370| lrm: 0.787447| num_tokens: 13,794
Step 00150/00701 | Training loss: 0.971892| lrm: 0.786020| num_tokens: 15,562
Step 00151/00701 | Training loss: 0.780729| lrm: 0.784593| num_tokens: 8,725
Step 00152/00701 | Training loss: 0.861489| lrm: 0.783167| num_tokens: 11,089
Step 00153/00701 | Training loss: 0.639376| lrm: 0.781740| num_tokens: 14,182
Step 00154/00701 | Training loss: 0.421294| lrm: 0.780314| num_tokens: 9,020
Step 00155/00701 | Training loss: 0.493460| lrm: 0.778887| num_tokens: 9,869
Step 00156/00701 | Training loss: 0.769833| lrm: 0.777461| num_tokens: 7,305
Step 00157/00701 | Training loss: 0.724690| lrm: 0.776034| num_tokens: 16,837
Step 00158/00701 | Training loss: 0.388184| lrm: 0.774608| num_tokens: 9,674
Step 00159/00701 | Training loss: 0.472026| lrm: 0.773181| num_tokens: 7,815
Step 00160/00701 | Training loss: 0.934271| lrm: 0.771755| num_tokens: 8,827
Step 00161/00701 | Training loss: 0.713041| lrm: 0.770328| num_tokens: 6,078
Step 00162/00701 | Training loss: 0.762265| lrm: 0.768902| num_tokens: 12,614
Step 00163/00701 | Training loss: 0.965620| lrm: 0.767475| num_tokens: 8,642
Step 00164/00701 | Training loss: 0.527537| lrm: 0.766049| num_tokens: 10,446
Step 00165/00701 | Training loss: 0.309202| lrm: 0.764622| num_tokens: 11,629
Step 00166/00701 | Training loss: 0.826532| lrm: 0.763195| num_tokens: 11,805
Step 00167/00701 | Training loss: 0.724818| lrm: 0.761769| num_tokens: 12,521
Step 00168/00701 | Training loss: 0.640260| lrm: 0.760342| num_tokens: 10,685
Step 00169/00701 | Training loss: 0.516270| lrm: 0.758916| num_tokens: 7,975
Step 00170/00701 | Training loss: 0.668462| lrm: 0.757489| num_tokens: 12,674
Step 00171/00701 | Training loss: 0.463269| lrm: 0.756063| num_tokens: 8,519
Step 00172/00701 | Training loss: 0.575599| lrm: 0.754636| num_tokens: 10,573
Step 00173/00701 | Training loss: 0.571729| lrm: 0.753210| num_tokens: 11,616
Step 00174/00701 | Training loss: 0.818436| lrm: 0.751783| num_tokens: 12,576
Step 00175/00701 | Training loss: 0.740733| lrm: 0.750357| num_tokens: 13,499
Step 00176/00701 | Training loss: 0.481243| lrm: 0.748930| num_tokens: 13,955
Step 00177/00701 | Training loss: 1.023409| lrm: 0.747504| num_tokens: 8,450
Step 00178/00701 | Training loss: 0.493217| lrm: 0.746077| num_tokens: 10,116
Step 00179/00701 | Training loss: 0.902471| lrm: 0.744650| num_tokens: 11,378
Step 00180/00701 | Training loss: 0.392506| lrm: 0.743224| num_tokens: 8,662
Step 00181/00701 | Training loss: 0.451104| lrm: 0.741797| num_tokens: 10,813
Step 00182/00701 | Training loss: 0.386739| lrm: 0.740371| num_tokens: 9,754
Step 00183/00701 | Training loss: 0.718970| lrm: 0.738944| num_tokens: 9,181
Step 00184/00701 | Training loss: 0.555858| lrm: 0.737518| num_tokens: 11,880
Step 00185/00701 | Training loss: 0.779567| lrm: 0.736091| num_tokens: 9,966
Step 00186/00701 | Training loss: 0.867234| lrm: 0.734665| num_tokens: 12,893
Step 00187/00701 | Training loss: 0.579400| lrm: 0.733238| num_tokens: 15,448
Step 00188/00701 | Training loss: 0.667884| lrm: 0.731812| num_tokens: 10,568
Step 00189/00701 | Training loss: 0.195775| lrm: 0.730385| num_tokens: 8,279
Step 00190/00701 | Training loss: 0.728171| lrm: 0.728959| num_tokens: 10,969
Step 00191/00701 | Training loss: 0.666785| lrm: 0.727532| num_tokens: 12,417
Step 00192/00701 | Training loss: 0.749913| lrm: 0.726106| num_tokens: 11,004
Step 00193/00701 | Training loss: 0.935838| lrm: 0.724679| num_tokens: 12,676
Step 00194/00701 | Training loss: 0.561135| lrm: 0.723252| num_tokens: 11,245
Step 00195/00701 | Training loss: 0.620850| lrm: 0.721826| num_tokens: 12,289
Step 00196/00701 | Training loss: 0.565862| lrm: 0.720399| num_tokens: 12,501
Step 00197/00701 | Training loss: 0.352398| lrm: 0.718973| num_tokens: 10,050
Step 00198/00701 | Training loss: 1.024213| lrm: 0.717546| num_tokens: 12,994
Step 00199/00701 | Training loss: 0.460693| lrm: 0.716120| num_tokens: 8,922
Step 00200 | Validation loss: 0.801521
Final: 410/1024 (40.04%)
Final: 400/570 (70.18%)
Step 00200 | mmlu_acc: 0.400391, arc_easy_acc: 0.701754
Step 00200/00701 | Training loss: 0.653739| lrm: 0.714693| num_tokens: 9,440
Step 00201/00701 | Training loss: 0.638369| lrm: 0.713267| num_tokens: 9,643
Step 00202/00701 | Training loss: 0.862583| lrm: 0.711840| num_tokens: 10,901
Step 00203/00701 | Training loss: 0.549880| lrm: 0.710414| num_tokens: 13,807
Step 00204/00701 | Training loss: 0.791708| lrm: 0.708987| num_tokens: 10,563
Step 00205/00701 | Training loss: 0.664582| lrm: 0.707561| num_tokens: 11,205
Step 00206/00701 | Training loss: 1.096743| lrm: 0.706134| num_tokens: 10,684
Step 00207/00701 | Training loss: 0.498752| lrm: 0.704708| num_tokens: 9,403
Step 00208/00701 | Training loss: 0.637192| lrm: 0.703281| num_tokens: 10,858
Step 00209/00701 | Training loss: 0.677578| lrm: 0.701854| num_tokens: 13,334
Step 00210/00701 | Training loss: 0.757477| lrm: 0.700428| num_tokens: 9,714
Step 00211/00701 | Training loss: 0.864387| lrm: 0.699001| num_tokens: 11,816
Step 00212/00701 | Training loss: 0.659707| lrm: 0.697575| num_tokens: 9,039
Step 00213/00701 | Training loss: 0.526195| lrm: 0.696148| num_tokens: 13,283
Step 00214/00701 | Training loss: 0.469802| lrm: 0.694722| num_tokens: 7,439
Step 00215/00701 | Training loss: 0.789171| lrm: 0.693295| num_tokens: 12,998
Step 00216/00701 | Training loss: 0.743867| lrm: 0.691869| num_tokens: 5,895
Step 00217/00701 | Training loss: 0.978305| lrm: 0.690442| num_tokens: 10,032
Step 00218/00701 | Training loss: 0.666577| lrm: 0.689016| num_tokens: 15,726
Step 00219/00701 | Training loss: 0.963608| lrm: 0.687589| num_tokens: 12,353
Step 00220/00701 | Training loss: 0.571761| lrm: 0.686163| num_tokens: 12,212
Step 00221/00701 | Training loss: 0.376570| lrm: 0.684736| num_tokens: 8,406
Step 00222/00701 | Training loss: 0.698827| lrm: 0.683310| num_tokens: 12,544
Step 00223/00701 | Training loss: 0.822297| lrm: 0.681883| num_tokens: 10,457
Step 00224/00701 | Training loss: 0.838259| lrm: 0.680456| num_tokens: 11,565
Step 00225/00701 | Training loss: 0.548999| lrm: 0.679030| num_tokens: 16,013
Step 00226/00701 | Training loss: 0.610505| lrm: 0.677603| num_tokens: 11,760
Step 00227/00701 | Training loss: 0.784591| lrm: 0.676177| num_tokens: 10,449
Step 00228/00701 | Training loss: 0.474747| lrm: 0.674750| num_tokens: 9,069
Step 00229/00701 | Training loss: 0.677853| lrm: 0.673324| num_tokens: 15,774
Step 00230/00701 | Training loss: 0.665943| lrm: 0.671897| num_tokens: 10,999
Step 00231/00701 | Training loss: 0.870985| lrm: 0.670471| num_tokens: 7,649
Step 00232/00701 | Training loss: 0.160847| lrm: 0.669044| num_tokens: 13,775
Step 00233/00701 | Training loss: 0.822165| lrm: 0.667618| num_tokens: 10,403
Step 00234/00701 | Training loss: 0.582248| lrm: 0.666191| num_tokens: 9,951
Step 00235/00701 | Training loss: 0.579611| lrm: 0.664765| num_tokens: 8,922
Step 00236/00701 | Training loss: 0.462333| lrm: 0.663338| num_tokens: 9,954
Step 00237/00701 | Training loss: 0.511807| lrm: 0.661912| num_tokens: 6,118
Step 00238/00701 | Training loss: 0.715136| lrm: 0.660485| num_tokens: 15,364
Step 00239/00701 | Training loss: 0.730765| lrm: 0.659058| num_tokens: 10,791
Step 00240/00701 | Training loss: 0.319264| lrm: 0.657632| num_tokens: 13,784
Step 00241/00701 | Training loss: 0.350992| lrm: 0.656205| num_tokens: 12,648
Step 00242/00701 | Training loss: 0.870052| lrm: 0.654779| num_tokens: 8,548
Step 00243/00701 | Training loss: 0.680624| lrm: 0.653352| num_tokens: 10,775
Step 00244/00701 | Training loss: 0.916721| lrm: 0.651926| num_tokens: 11,321
Step 00245/00701 | Training loss: 0.898496| lrm: 0.650499| num_tokens: 14,500
Step 00246/00701 | Training loss: 0.434766| lrm: 0.649073| num_tokens: 9,928
Step 00247/00701 | Training loss: 0.294292| lrm: 0.647646| num_tokens: 9,820
Step 00248/00701 | Training loss: 0.694202| lrm: 0.646220| num_tokens: 14,648
Step 00249/00701 | Training loss: 0.834129| lrm: 0.644793| num_tokens: 12,353
Step 00250/00701 | Training loss: 0.437915| lrm: 0.643367| num_tokens: 10,111
Step 00251/00701 | Training loss: 0.887474| lrm: 0.641940| num_tokens: 9,723
Step 00252/00701 | Training loss: 0.732791| lrm: 0.640514| num_tokens: 13,470
Step 00253/00701 | Training loss: 0.658651| lrm: 0.639087| num_tokens: 8,697
Step 00254/00701 | Training loss: 0.753637| lrm: 0.637660| num_tokens: 10,005
Step 00255/00701 | Training loss: 0.234044| lrm: 0.636234| num_tokens: 9,406
Step 00256/00701 | Training loss: 0.663994| lrm: 0.634807| num_tokens: 16,524
Step 00257/00701 | Training loss: 0.629444| lrm: 0.633381| num_tokens: 10,533
Step 00258/00701 | Training loss: 0.573985| lrm: 0.631954| num_tokens: 14,366
Step 00259/00701 | Training loss: 0.689693| lrm: 0.630528| num_tokens: 10,793
Step 00260/00701 | Training loss: 0.642471| lrm: 0.629101| num_tokens: 9,875
Step 00261/00701 | Training loss: 0.783415| lrm: 0.627675| num_tokens: 9,230
Step 00262/00701 | Training loss: 0.807046| lrm: 0.626248| num_tokens: 7,452
Step 00263/00701 | Training loss: 0.787288| lrm: 0.624822| num_tokens: 11,783
Step 00264/00701 | Training loss: 0.404335| lrm: 0.623395| num_tokens: 9,665
Step 00265/00701 | Training loss: 0.792673| lrm: 0.621969| num_tokens: 9,502
Step 00266/00701 | Training loss: 1.067547| lrm: 0.620542| num_tokens: 13,734
Step 00267/00701 | Training loss: 0.591969| lrm: 0.619116| num_tokens: 13,450
Step 00268/00701 | Training loss: 0.772576| lrm: 0.617689| num_tokens: 14,942
Step 00269/00701 | Training loss: 0.363730| lrm: 0.616262| num_tokens: 8,918
Step 00270/00701 | Training loss: 0.310721| lrm: 0.614836| num_tokens: 13,811
Step 00271/00701 | Training loss: 1.026305| lrm: 0.613409| num_tokens: 7,419
Step 00272/00701 | Training loss: 0.776614| lrm: 0.611983| num_tokens: 9,250
Step 00273/00701 | Training loss: 0.783950| lrm: 0.610556| num_tokens: 8,725
Step 00274/00701 | Training loss: 0.669959| lrm: 0.609130| num_tokens: 14,207
Step 00275/00701 | Training loss: 0.834995| lrm: 0.607703| num_tokens: 12,263
Step 00276/00701 | Training loss: 0.598181| lrm: 0.606277| num_tokens: 11,752
Step 00277/00701 | Training loss: 0.634589| lrm: 0.604850| num_tokens: 12,410
Step 00278/00701 | Training loss: 0.793521| lrm: 0.603424| num_tokens: 12,400
Step 00279/00701 | Training loss: 0.452058| lrm: 0.601997| num_tokens: 9,813
Step 00280/00701 | Training loss: 0.491893| lrm: 0.600571| num_tokens: 8,658
Step 00281/00701 | Training loss: 0.446493| lrm: 0.599144| num_tokens: 10,995
Step 00282/00701 | Training loss: 0.563347| lrm: 0.597718| num_tokens: 12,312
Step 00283/00701 | Training loss: 0.438849| lrm: 0.596291| num_tokens: 11,610
Step 00284/00701 | Training loss: 0.625154| lrm: 0.594864| num_tokens: 13,601
Step 00285/00701 | Training loss: 0.189336| lrm: 0.593438| num_tokens: 10,879
Step 00286/00701 | Training loss: 0.583846| lrm: 0.592011| num_tokens: 10,792
Step 00287/00701 | Training loss: 0.376244| lrm: 0.590585| num_tokens: 16,421
Step 00288/00701 | Training loss: 0.504632| lrm: 0.589158| num_tokens: 10,023
Step 00289/00701 | Training loss: 0.558860| lrm: 0.587732| num_tokens: 13,373
Step 00290/00701 | Training loss: 0.538424| lrm: 0.586305| num_tokens: 8,400
Step 00291/00701 | Training loss: 0.877753| lrm: 0.584879| num_tokens: 11,983
Step 00292/00701 | Training loss: 0.789233| lrm: 0.583452| num_tokens: 13,546
Step 00293/00701 | Training loss: 0.733271| lrm: 0.582026| num_tokens: 14,886
Step 00294/00701 | Training loss: 0.665984| lrm: 0.580599| num_tokens: 9,826
Step 00295/00701 | Training loss: 0.610238| lrm: 0.579173| num_tokens: 6,348
Step 00296/00701 | Training loss: 0.885608| lrm: 0.577746| num_tokens: 9,937
Step 00297/00701 | Training loss: 0.339954| lrm: 0.576320| num_tokens: 15,858
Step 00298/00701 | Training loss: 0.544652| lrm: 0.574893| num_tokens: 12,569
Step 00299/00701 | Training loss: 0.862469| lrm: 0.573466| num_tokens: 10,266
Step 00300 | Validation loss: 0.801380
Step 00300/00701 | Training loss: 0.617219| lrm: 0.572040| num_tokens: 9,665
Step 00301/00701 | Training loss: 0.534649| lrm: 0.570613| num_tokens: 10,204
Step 00302/00701 | Training loss: 0.563877| lrm: 0.569187| num_tokens: 10,878
Step 00303/00701 | Training loss: 0.606976| lrm: 0.567760| num_tokens: 7,932
Step 00304/00701 | Training loss: 0.517978| lrm: 0.566334| num_tokens: 13,879
Step 00305/00701 | Training loss: 0.463168| lrm: 0.564907| num_tokens: 14,422
Step 00306/00701 | Training loss: 0.743737| lrm: 0.563481| num_tokens: 13,821
Step 00307/00701 | Training loss: 0.590056| lrm: 0.562054| num_tokens: 15,326
Step 00308/00701 | Training loss: 0.530959| lrm: 0.560628| num_tokens: 7,481
Step 00309/00701 | Training loss: 0.759154| lrm: 0.559201| num_tokens: 10,303
Step 00310/00701 | Training loss: 0.566835| lrm: 0.557775| num_tokens: 11,936
Step 00311/00701 | Training loss: 0.516546| lrm: 0.556348| num_tokens: 9,810
Step 00312/00701 | Training loss: 0.497984| lrm: 0.554922| num_tokens: 12,527
Step 00313/00701 | Training loss: 0.800699| lrm: 0.553495| num_tokens: 8,958
Step 00314/00701 | Training loss: 0.650904| lrm: 0.552068| num_tokens: 13,602
Step 00315/00701 | Training loss: 0.737872| lrm: 0.550642| num_tokens: 11,829
Step 00316/00701 | Training loss: 0.633959| lrm: 0.549215| num_tokens: 10,127
Step 00317/00701 | Training loss: 0.717965| lrm: 0.547789| num_tokens: 15,057
Step 00318/00701 | Training loss: 0.582311| lrm: 0.546362| num_tokens: 12,107
Step 00319/00701 | Training loss: 0.704487| lrm: 0.544936| num_tokens: 13,207
Step 00320/00701 | Training loss: 0.258423| lrm: 0.543509| num_tokens: 9,324
Step 00321/00701 | Training loss: 0.684822| lrm: 0.542083| num_tokens: 8,402
Step 00322/00701 | Training loss: 0.649416| lrm: 0.540656| num_tokens: 9,564
Step 00323/00701 | Training loss: 0.754423| lrm: 0.539230| num_tokens: 14,255
Step 00324/00701 | Training loss: 0.441876| lrm: 0.537803| num_tokens: 9,640
Step 00325/00701 | Training loss: 0.468193| lrm: 0.536377| num_tokens: 10,153
Step 00326/00701 | Training loss: 0.824618| lrm: 0.534950| num_tokens: 5,101
Step 00327/00701 | Training loss: 0.621810| lrm: 0.533524| num_tokens: 12,290
Step 00328/00701 | Training loss: 0.512864| lrm: 0.532097| num_tokens: 13,156
Step 00329/00701 | Training loss: 0.807332| lrm: 0.530670| num_tokens: 12,385
Step 00330/00701 | Training loss: 0.832426| lrm: 0.529244| num_tokens: 8,812
Step 00331/00701 | Training loss: 0.862390| lrm: 0.527817| num_tokens: 8,644
Step 00332/00701 | Training loss: 0.606254| lrm: 0.526391| num_tokens: 13,018
Step 00333/00701 | Training loss: 0.625257| lrm: 0.524964| num_tokens: 12,307
Step 00334/00701 | Training loss: 0.339412| lrm: 0.523538| num_tokens: 11,387
Step 00335/00701 | Training loss: 0.416131| lrm: 0.522111| num_tokens: 10,716
Step 00336/00701 | Training loss: 0.622982| lrm: 0.520685| num_tokens: 13,678
Step 00337/00701 | Training loss: 0.553735| lrm: 0.519258| num_tokens: 10,665
Step 00338/00701 | Training loss: 0.976349| lrm: 0.517832| num_tokens: 9,418
Step 00339/00701 | Training loss: 0.734651| lrm: 0.516405| num_tokens: 12,199
Step 00340/00701 | Training loss: 0.716402| lrm: 0.514979| num_tokens: 9,814
Step 00341/00701 | Training loss: 0.947899| lrm: 0.513552| num_tokens: 11,241
Step 00342/00701 | Training loss: 0.450224| lrm: 0.512126| num_tokens: 8,510
Step 00343/00701 | Training loss: 0.340320| lrm: 0.510699| num_tokens: 8,510
Step 00344/00701 | Training loss: 0.455404| lrm: 0.509272| num_tokens: 6,761
Step 00345/00701 | Training loss: 0.546307| lrm: 0.507846| num_tokens: 11,059
Step 00346/00701 | Training loss: 0.403309| lrm: 0.506419| num_tokens: 14,718
Step 00347/00701 | Training loss: 0.305629| lrm: 0.504993| num_tokens: 11,942
Step 00348/00701 | Training loss: 0.521121| lrm: 0.503566| num_tokens: 10,405
Step 00349/00701 | Training loss: 0.755553| lrm: 0.502140| num_tokens: 9,043
Step 00350/00701 | Training loss: 0.397839| lrm: 0.500713| num_tokens: 10,832
Step 00351/00701 | Training loss: 1.173707| lrm: 0.499287| num_tokens: 8,975
Step 00352/00701 | Training loss: 0.756448| lrm: 0.497860| num_tokens: 11,159
Step 00353/00701 | Training loss: 1.043745| lrm: 0.496434| num_tokens: 8,952
Step 00354/00701 | Training loss: 0.393402| lrm: 0.495007| num_tokens: 6,602
Step 00355/00701 | Training loss: 0.505306| lrm: 0.493581| num_tokens: 10,783
Step 00356/00701 | Training loss: 0.640673| lrm: 0.492154| num_tokens: 7,397
Step 00357/00701 | Training loss: 0.604385| lrm: 0.490728| num_tokens: 8,771
Step 00358/00701 | Training loss: 0.420553| lrm: 0.489301| num_tokens: 12,280
Step 00359/00701 | Training loss: 0.378189| lrm: 0.487874| num_tokens: 8,768
Step 00360/00701 | Training loss: 0.699019| lrm: 0.486448| num_tokens: 6,184
Step 00361/00701 | Training loss: 0.237319| lrm: 0.485021| num_tokens: 9,414
Step 00362/00701 | Training loss: 0.455122| lrm: 0.483595| num_tokens: 9,993
Step 00363/00701 | Training loss: 0.809406| lrm: 0.482168| num_tokens: 13,452
Step 00364/00701 | Training loss: 0.314640| lrm: 0.480742| num_tokens: 9,705
Step 00365/00701 | Training loss: 0.619671| lrm: 0.479315| num_tokens: 14,972
Step 00366/00701 | Training loss: 0.872663| lrm: 0.477889| num_tokens: 9,769
Step 00367/00701 | Training loss: 0.645485| lrm: 0.476462| num_tokens: 12,077
Step 00368/00701 | Training loss: 0.690516| lrm: 0.475036| num_tokens: 10,736
Step 00369/00701 | Training loss: 0.926094| lrm: 0.473609| num_tokens: 8,677
Step 00370/00701 | Training loss: 0.628869| lrm: 0.472183| num_tokens: 10,759
Step 00371/00701 | Training loss: 0.434099| lrm: 0.470756| num_tokens: 14,669
Step 00372/00701 | Training loss: 0.681287| lrm: 0.469330| num_tokens: 9,155
Step 00373/00701 | Training loss: 0.669175| lrm: 0.467903| num_tokens: 12,243
Step 00374/00701 | Training loss: 0.670543| lrm: 0.466476| num_tokens: 12,642
Step 00375/00701 | Training loss: 0.733350| lrm: 0.465050| num_tokens: 8,785
Step 00376/00701 | Training loss: 0.832549| lrm: 0.463623| num_tokens: 11,271
Step 00377/00701 | Training loss: 0.544078| lrm: 0.462197| num_tokens: 19,989
Step 00378/00701 | Training loss: 0.714742| lrm: 0.460770| num_tokens: 12,043
Step 00379/00701 | Training loss: 0.538047| lrm: 0.459344| num_tokens: 8,660
Step 00380/00701 | Training loss: 0.660399| lrm: 0.457917| num_tokens: 10,150
Step 00381/00701 | Training loss: 0.602761| lrm: 0.456491| num_tokens: 12,855
Step 00382/00701 | Training loss: 0.470806| lrm: 0.455064| num_tokens: 13,113
Step 00383/00701 | Training loss: 0.722205| lrm: 0.453638| num_tokens: 11,984
Step 00384/00701 | Training loss: 1.019692| lrm: 0.452211| num_tokens: 14,243
Step 00385/00701 | Training loss: 0.579287| lrm: 0.450785| num_tokens: 7,839
Step 00386/00701 | Training loss: 0.517784| lrm: 0.449358| num_tokens: 14,076
Step 00387/00701 | Training loss: 0.562769| lrm: 0.447932| num_tokens: 9,040
Step 00388/00701 | Training loss: 0.501534| lrm: 0.446505| num_tokens: 11,169
Step 00389/00701 | Training loss: 0.516283| lrm: 0.445078| num_tokens: 13,358
Step 00390/00701 | Training loss: 0.636603| lrm: 0.443652| num_tokens: 13,180
Step 00391/00701 | Training loss: 0.579672| lrm: 0.442225| num_tokens: 12,662
Step 00392/00701 | Training loss: 0.745333| lrm: 0.440799| num_tokens: 11,049
Step 00393/00701 | Training loss: 0.678952| lrm: 0.439372| num_tokens: 13,068
Step 00394/00701 | Training loss: 0.609830| lrm: 0.437946| num_tokens: 13,400
Step 00395/00701 | Training loss: 0.693325| lrm: 0.436519| num_tokens: 8,514
Step 00396/00701 | Training loss: 0.349515| lrm: 0.435093| num_tokens: 11,226
Step 00397/00701 | Training loss: 0.810564| lrm: 0.433666| num_tokens: 10,475
Step 00398/00701 | Training loss: 0.688904| lrm: 0.432240| num_tokens: 8,499
Step 00399/00701 | Training loss: 0.614489| lrm: 0.430813| num_tokens: 7,556
Step 00400 | Validation loss: 0.800674
Final: 413/1024 (40.33%)
Final: 409/570 (71.75%)
Step 00400 | mmlu_acc: 0.403320, arc_easy_acc: 0.717544
Step 00400/00701 | Training loss: 0.568205| lrm: 0.429387| num_tokens: 14,731
Step 00401/00701 | Training loss: 0.446347| lrm: 0.427960| num_tokens: 6,824
Step 00402/00701 | Training loss: 0.455932| lrm: 0.426534| num_tokens: 7,615
Step 00403/00701 | Training loss: 0.747580| lrm: 0.425107| num_tokens: 14,374
Step 00404/00701 | Training loss: 0.815379| lrm: 0.423680| num_tokens: 9,695
Step 00405/00701 | Training loss: 0.485525| lrm: 0.422254| num_tokens: 9,349
Step 00406/00701 | Training loss: 0.742281| lrm: 0.420827| num_tokens: 13,959
Step 00407/00701 | Training loss: 0.371894| lrm: 0.419401| num_tokens: 8,434
Step 00408/00701 | Training loss: 0.746686| lrm: 0.417974| num_tokens: 13,160
Step 00409/00701 | Training loss: 0.680421| lrm: 0.416548| num_tokens: 10,531
Step 00410/00701 | Training loss: 0.463686| lrm: 0.415121| num_tokens: 8,332
Step 00411/00701 | Training loss: 0.616889| lrm: 0.413695| num_tokens: 12,826
Step 00412/00701 | Training loss: 0.792985| lrm: 0.412268| num_tokens: 10,446
Step 00413/00701 | Training loss: 0.929220| lrm: 0.410842| num_tokens: 9,275
Step 00414/00701 | Training loss: 0.624524| lrm: 0.409415| num_tokens: 11,398
Step 00415/00701 | Training loss: 0.265304| lrm: 0.407989| num_tokens: 9,138
Step 00416/00701 | Training loss: 0.439725| lrm: 0.406562| num_tokens: 11,156
Step 00417/00701 | Training loss: 0.590525| lrm: 0.405136| num_tokens: 13,939
Step 00418/00701 | Training loss: 0.603561| lrm: 0.403709| num_tokens: 7,993
Step 00419/00701 | Training loss: 0.649206| lrm: 0.402282| num_tokens: 10,453
Step 00420/00701 | Training loss: 0.800661| lrm: 0.400856| num_tokens: 8,721
Step 00421/00701 | Training loss: 0.435670| lrm: 0.399429| num_tokens: 9,896
Step 00422/00701 | Training loss: 0.816047| lrm: 0.398003| num_tokens: 12,520
Step 00423/00701 | Training loss: 0.615700| lrm: 0.396576| num_tokens: 10,323
Step 00424/00701 | Training loss: 0.379932| lrm: 0.395150| num_tokens: 16,153
Step 00425/00701 | Training loss: 0.708881| lrm: 0.393723| num_tokens: 6,174
Step 00426/00701 | Training loss: 0.554216| lrm: 0.392297| num_tokens: 11,888
Step 00427/00701 | Training loss: 0.587017| lrm: 0.390870| num_tokens: 10,906
Step 00428/00701 | Training loss: 0.535607| lrm: 0.389444| num_tokens: 9,491
Step 00429/00701 | Training loss: 0.295649| lrm: 0.388017| num_tokens: 9,577
Step 00430/00701 | Training loss: 0.546575| lrm: 0.386591| num_tokens: 9,715
Step 00431/00701 | Training loss: 0.644760| lrm: 0.385164| num_tokens: 11,295
Step 00432/00701 | Training loss: 0.656178| lrm: 0.383738| num_tokens: 10,906
Step 00433/00701 | Training loss: 0.610051| lrm: 0.382311| num_tokens: 11,956
Step 00434/00701 | Training loss: 0.706071| lrm: 0.380884| num_tokens: 10,384
Step 00435/00701 | Training loss: 0.374482| lrm: 0.379458| num_tokens: 15,339
Step 00436/00701 | Training loss: 0.440274| lrm: 0.378031| num_tokens: 11,631
Step 00437/00701 | Training loss: 0.538097| lrm: 0.376605| num_tokens: 11,177
Step 00438/00701 | Training loss: 0.644399| lrm: 0.375178| num_tokens: 9,906
Step 00439/00701 | Training loss: 0.373050| lrm: 0.373752| num_tokens: 20,036
Step 00440/00701 | Training loss: 0.633737| lrm: 0.372325| num_tokens: 9,024
Step 00441/00701 | Training loss: 0.746535| lrm: 0.370899| num_tokens: 12,635
Step 00442/00701 | Training loss: 0.858692| lrm: 0.369472| num_tokens: 12,312
Step 00443/00701 | Training loss: 0.726897| lrm: 0.368046| num_tokens: 9,621
Step 00444/00701 | Training loss: 0.527249| lrm: 0.366619| num_tokens: 8,084
Step 00445/00701 | Training loss: 0.764821| lrm: 0.365193| num_tokens: 9,844
Step 00446/00701 | Training loss: 0.383286| lrm: 0.363766| num_tokens: 12,791
Step 00447/00701 | Training loss: 0.611870| lrm: 0.362340| num_tokens: 13,150
Step 00448/00701 | Training loss: 0.197127| lrm: 0.360913| num_tokens: 13,924
Step 00449/00701 | Training loss: 0.390552| lrm: 0.359486| num_tokens: 10,550
Step 00450/00701 | Training loss: 0.835925| lrm: 0.358060| num_tokens: 10,879
Step 00451/00701 | Training loss: 0.422442| lrm: 0.356633| num_tokens: 8,315
Step 00452/00701 | Training loss: 0.637335| lrm: 0.355207| num_tokens: 12,395
Step 00453/00701 | Training loss: 1.109235| lrm: 0.353780| num_tokens: 15,440
Step 00454/00701 | Training loss: 0.661626| lrm: 0.352354| num_tokens: 8,555
Step 00455/00701 | Training loss: 0.717379| lrm: 0.350927| num_tokens: 10,449
Step 00456/00701 | Training loss: 0.415701| lrm: 0.349501| num_tokens: 12,364
Step 00457/00701 | Training loss: 0.498651| lrm: 0.348074| num_tokens: 11,084
Step 00458/00701 | Training loss: 0.421625| lrm: 0.346648| num_tokens: 10,220
Step 00459/00701 | Training loss: 0.646383| lrm: 0.345221| num_tokens: 10,655
Step 00460/00701 | Training loss: 0.726612| lrm: 0.343795| num_tokens: 9,123
Step 00461/00701 | Training loss: 1.046222| lrm: 0.342368| num_tokens: 11,044
Step 00462/00701 | Training loss: 0.997698| lrm: 0.340942| num_tokens: 9,099
Step 00463/00701 | Training loss: 0.691863| lrm: 0.339515| num_tokens: 9,954
Step 00464/00701 | Training loss: 0.576442| lrm: 0.338088| num_tokens: 10,790
Step 00465/00701 | Training loss: 0.526879| lrm: 0.336662| num_tokens: 9,018
Step 00466/00701 | Training loss: 0.263657| lrm: 0.335235| num_tokens: 10,839
Step 00467/00701 | Training loss: 0.430071| lrm: 0.333809| num_tokens: 9,138
Step 00468/00701 | Training loss: 0.623628| lrm: 0.332382| num_tokens: 11,911
Step 00469/00701 | Training loss: 0.409929| lrm: 0.330956| num_tokens: 6,290
Step 00470/00701 | Training loss: 0.224719| lrm: 0.329529| num_tokens: 14,177
Step 00471/00701 | Training loss: 0.474796| lrm: 0.328103| num_tokens: 12,463
Step 00472/00701 | Training loss: 0.534690| lrm: 0.326676| num_tokens: 11,607
Step 00473/00701 | Training loss: 0.462423| lrm: 0.325250| num_tokens: 10,416
Step 00474/00701 | Training loss: 0.810964| lrm: 0.323823| num_tokens: 12,980
Step 00475/00701 | Training loss: 0.660966| lrm: 0.322397| num_tokens: 12,434
Step 00476/00701 | Training loss: 0.663512| lrm: 0.320970| num_tokens: 13,188
Step 00477/00701 | Training loss: 0.461684| lrm: 0.319544| num_tokens: 12,126
Step 00478/00701 | Training loss: 0.831516| lrm: 0.318117| num_tokens: 11,323
Step 00479/00701 | Training loss: 0.698021| lrm: 0.316690| num_tokens: 8,197
Step 00480/00701 | Training loss: 0.416182| lrm: 0.315264| num_tokens: 11,431
Step 00481/00701 | Training loss: 0.773171| lrm: 0.313837| num_tokens: 9,379
Step 00482/00701 | Training loss: 0.554658| lrm: 0.312411| num_tokens: 11,251
Step 00483/00701 | Training loss: 0.843574| lrm: 0.310984| num_tokens: 9,644
Step 00484/00701 | Training loss: 0.215098| lrm: 0.309558| num_tokens: 14,161
Step 00485/00701 | Training loss: 1.021553| lrm: 0.308131| num_tokens: 9,821
Step 00486/00701 | Training loss: 0.750114| lrm: 0.306705| num_tokens: 11,400
Step 00487/00701 | Training loss: 0.557213| lrm: 0.305278| num_tokens: 9,749
Step 00488/00701 | Training loss: 0.852862| lrm: 0.303852| num_tokens: 11,001
Step 00489/00701 | Training loss: 0.786608| lrm: 0.302425| num_tokens: 11,772
Step 00490/00701 | Training loss: 0.601803| lrm: 0.300999| num_tokens: 7,971
Step 00491/00701 | Training loss: 0.474517| lrm: 0.299572| num_tokens: 13,434
Step 00492/00701 | Training loss: 0.521257| lrm: 0.298146| num_tokens: 9,852
Step 00493/00701 | Training loss: 0.650273| lrm: 0.296719| num_tokens: 9,090
Step 00494/00701 | Training loss: 0.587404| lrm: 0.295292| num_tokens: 6,953
Step 00495/00701 | Training loss: 0.517347| lrm: 0.293866| num_tokens: 11,616
Step 00496/00701 | Training loss: 0.513381| lrm: 0.292439| num_tokens: 11,101
Step 00497/00701 | Training loss: 0.726402| lrm: 0.291013| num_tokens: 5,859
Step 00498/00701 | Training loss: 0.862282| lrm: 0.289586| num_tokens: 13,441
Step 00499/00701 | Training loss: 0.511069| lrm: 0.288160| num_tokens: 14,167
Step 00500 | Validation loss: 0.800744
Step 00500/00701 | Training loss: 0.600114| lrm: 0.286733| num_tokens: 14,698
Step 00501/00701 | Training loss: 0.481008| lrm: 0.285307| num_tokens: 11,011
Step 00502/00701 | Training loss: 0.673864| lrm: 0.283880| num_tokens: 7,577
Step 00503/00701 | Training loss: 0.701373| lrm: 0.282454| num_tokens: 11,184
Step 00504/00701 | Training loss: 0.293271| lrm: 0.281027| num_tokens: 15,243
Step 00505/00701 | Training loss: 0.486756| lrm: 0.279601| num_tokens: 7,160
Step 00506/00701 | Training loss: 0.768737| lrm: 0.278174| num_tokens: 10,936
Step 00507/00701 | Training loss: 0.781474| lrm: 0.276748| num_tokens: 9,774
Step 00508/00701 | Training loss: 0.541375| lrm: 0.275321| num_tokens: 14,230
Step 00509/00701 | Training loss: 0.362554| lrm: 0.273894| num_tokens: 10,653
Step 00510/00701 | Training loss: 0.442984| lrm: 0.272468| num_tokens: 13,249
Step 00511/00701 | Training loss: 0.665810| lrm: 0.271041| num_tokens: 11,578
Step 00512/00701 | Training loss: 0.688371| lrm: 0.269615| num_tokens: 12,354
Step 00513/00701 | Training loss: 0.567196| lrm: 0.268188| num_tokens: 12,883
Step 00514/00701 | Training loss: 0.378645| lrm: 0.266762| num_tokens: 9,849
Step 00515/00701 | Training loss: 0.844057| lrm: 0.265335| num_tokens: 10,643
Step 00516/00701 | Training loss: 0.849287| lrm: 0.263909| num_tokens: 14,285
Step 00517/00701 | Training loss: 0.203791| lrm: 0.262482| num_tokens: 12,810
Step 00518/00701 | Training loss: 0.666720| lrm: 0.261056| num_tokens: 10,528
Step 00519/00701 | Training loss: 0.702563| lrm: 0.259629| num_tokens: 8,934
Step 00520/00701 | Training loss: 1.100993| lrm: 0.258203| num_tokens: 8,914
Step 00521/00701 | Training loss: 0.675931| lrm: 0.256776| num_tokens: 12,352
Step 00522/00701 | Training loss: 0.665228| lrm: 0.255350| num_tokens: 14,011
Step 00523/00701 | Training loss: 0.441276| lrm: 0.253923| num_tokens: 13,005
Step 00524/00701 | Training loss: 0.484619| lrm: 0.252496| num_tokens: 9,338
Step 00525/00701 | Training loss: 0.767930| lrm: 0.251070| num_tokens: 13,444
Step 00526/00701 | Training loss: 0.751818| lrm: 0.249643| num_tokens: 8,556
Step 00527/00701 | Training loss: 0.744033| lrm: 0.248217| num_tokens: 10,442
Step 00528/00701 | Training loss: 0.989556| lrm: 0.246790| num_tokens: 13,699
Step 00529/00701 | Training loss: 0.636179| lrm: 0.245364| num_tokens: 17,914
Step 00530/00701 | Training loss: 1.047590| lrm: 0.243937| num_tokens: 9,459
Step 00531/00701 | Training loss: 0.626265| lrm: 0.242511| num_tokens: 13,064
Step 00532/00701 | Training loss: 0.382534| lrm: 0.241084| num_tokens: 8,146
Step 00533/00701 | Training loss: 0.218363| lrm: 0.239658| num_tokens: 14,209
Step 00534/00701 | Training loss: 0.324882| lrm: 0.238231| num_tokens: 14,990
Step 00535/00701 | Training loss: 0.511676| lrm: 0.236805| num_tokens: 12,819
Step 00536/00701 | Training loss: 0.402684| lrm: 0.235378| num_tokens: 9,796
Step 00537/00701 | Training loss: 0.452074| lrm: 0.233951| num_tokens: 7,492
Step 00538/00701 | Training loss: 0.433324| lrm: 0.232525| num_tokens: 8,982
Step 00539/00701 | Training loss: 0.550679| lrm: 0.231098| num_tokens: 10,290
Step 00540/00701 | Training loss: 0.810563| lrm: 0.229672| num_tokens: 10,607
Step 00541/00701 | Training loss: 0.506051| lrm: 0.228245| num_tokens: 14,682
Step 00542/00701 | Training loss: 0.516896| lrm: 0.226819| num_tokens: 9,701
Step 00543/00701 | Training loss: 0.643840| lrm: 0.225392| num_tokens: 7,453
Step 00544/00701 | Training loss: 0.392352| lrm: 0.223966| num_tokens: 8,799
Step 00545/00701 | Training loss: 0.597755| lrm: 0.222539| num_tokens: 15,993
Step 00546/00701 | Training loss: 0.714608| lrm: 0.221113| num_tokens: 15,408
Step 00547/00701 | Training loss: 1.345927| lrm: 0.219686| num_tokens: 9,156
Step 00548/00701 | Training loss: 0.517790| lrm: 0.218260| num_tokens: 12,673
Step 00549/00701 | Training loss: 0.294443| lrm: 0.216833| num_tokens: 9,264
Step 00550/00701 | Training loss: 0.278370| lrm: 0.215407| num_tokens: 14,445
Step 00551/00701 | Training loss: 0.647834| lrm: 0.213980| num_tokens: 6,409
Step 00552/00701 | Training loss: 0.666977| lrm: 0.212553| num_tokens: 15,682
Step 00553/00701 | Training loss: 0.321659| lrm: 0.211127| num_tokens: 11,319
Step 00554/00701 | Training loss: 0.748918| lrm: 0.209700| num_tokens: 8,213
Step 00555/00701 | Training loss: 0.556436| lrm: 0.208274| num_tokens: 10,998
Step 00556/00701 | Training loss: 0.908266| lrm: 0.206847| num_tokens: 7,816
Step 00557/00701 | Training loss: 0.713487| lrm: 0.205421| num_tokens: 10,059
Step 00558/00701 | Training loss: 0.448224| lrm: 0.203994| num_tokens: 11,108
Step 00559/00701 | Training loss: 0.606619| lrm: 0.202568| num_tokens: 9,751
Step 00560/00701 | Training loss: 0.450164| lrm: 0.201141| num_tokens: 12,662
Step 00561/00701 | Training loss: 0.857579| lrm: 0.199715| num_tokens: 8,694
Step 00562/00701 | Training loss: 0.324890| lrm: 0.198288| num_tokens: 10,830
Step 00563/00701 | Training loss: 0.562574| lrm: 0.196862| num_tokens: 11,581
Step 00564/00701 | Training loss: 0.688146| lrm: 0.195435| num_tokens: 12,720
Step 00565/00701 | Training loss: 0.656145| lrm: 0.194009| num_tokens: 10,999
Step 00566/00701 | Training loss: 0.690613| lrm: 0.192582| num_tokens: 14,838
Step 00567/00701 | Training loss: 0.503092| lrm: 0.191155| num_tokens: 14,459
Step 00568/00701 | Training loss: 0.405053| lrm: 0.189729| num_tokens: 12,821
Step 00569/00701 | Training loss: 0.852091| lrm: 0.188302| num_tokens: 7,838
Step 00570/00701 | Training loss: 0.633516| lrm: 0.186876| num_tokens: 11,563
Step 00571/00701 | Training loss: 0.646007| lrm: 0.185449| num_tokens: 8,627
Step 00572/00701 | Training loss: 0.825594| lrm: 0.184023| num_tokens: 15,384
Step 00573/00701 | Training loss: 0.544473| lrm: 0.182596| num_tokens: 9,452
Step 00574/00701 | Training loss: 0.674340| lrm: 0.181170| num_tokens: 11,095
Step 00575/00701 | Training loss: 0.649242| lrm: 0.179743| num_tokens: 11,208
Step 00576/00701 | Training loss: 0.592174| lrm: 0.178317| num_tokens: 15,271
Step 00577/00701 | Training loss: 0.271633| lrm: 0.176890| num_tokens: 15,653
Step 00578/00701 | Training loss: 0.479086| lrm: 0.175464| num_tokens: 10,777
Step 00579/00701 | Training loss: 0.555440| lrm: 0.174037| num_tokens: 12,880
Step 00580/00701 | Training loss: 0.327925| lrm: 0.172611| num_tokens: 9,843
Step 00581/00701 | Training loss: 0.616981| lrm: 0.171184| num_tokens: 15,024
Step 00582/00701 | Training loss: 0.483394| lrm: 0.169757| num_tokens: 10,680
Step 00583/00701 | Training loss: 1.297850| lrm: 0.168331| num_tokens: 11,704
Step 00584/00701 | Training loss: 0.079000| lrm: 0.166904| num_tokens: 7,994
Step 00585/00701 | Training loss: 0.629320| lrm: 0.165478| num_tokens: 13,732
Step 00586/00701 | Training loss: 0.516178| lrm: 0.164051| num_tokens: 12,760
Step 00587/00701 | Training loss: 0.436512| lrm: 0.162625| num_tokens: 8,403
Step 00588/00701 | Training loss: 0.824986| lrm: 0.161198| num_tokens: 15,870
Step 00589/00701 | Training loss: 1.434506| lrm: 0.159772| num_tokens: 11,288
Step 00590/00701 | Training loss: 0.559604| lrm: 0.158345| num_tokens: 10,763
Step 00591/00701 | Training loss: 0.549657| lrm: 0.156919| num_tokens: 11,348
Step 00592/00701 | Training loss: 0.783768| lrm: 0.155492| num_tokens: 6,756
Step 00593/00701 | Training loss: 0.577267| lrm: 0.154066| num_tokens: 9,081
Step 00594/00701 | Training loss: 0.637744| lrm: 0.152639| num_tokens: 9,289
Step 00595/00701 | Training loss: 0.803318| lrm: 0.151213| num_tokens: 9,772
Step 00596/00701 | Training loss: 0.834137| lrm: 0.149786| num_tokens: 9,154
Step 00597/00701 | Training loss: 0.533900| lrm: 0.148359| num_tokens: 10,072
Step 00598/00701 | Training loss: 0.639542| lrm: 0.146933| num_tokens: 11,714
Step 00599/00701 | Training loss: 0.803376| lrm: 0.145506| num_tokens: 10,235
Step 00600 | Validation loss: 0.800404
Final: 414/1024 (40.43%)
Final: 408/570 (71.58%)
Step 00600 | mmlu_acc: 0.404297, arc_easy_acc: 0.715789
Step 00600/00701 | Training loss: 0.916912| lrm: 0.144080| num_tokens: 9,938
Step 00601/00701 | Training loss: 0.548929| lrm: 0.142653| num_tokens: 12,145
Step 00602/00701 | Training loss: 0.630325| lrm: 0.141227| num_tokens: 8,811
Step 00603/00701 | Training loss: 0.507950| lrm: 0.139800| num_tokens: 9,813
Step 00604/00701 | Training loss: 0.581703| lrm: 0.138374| num_tokens: 14,253
Step 00605/00701 | Training loss: 0.983156| lrm: 0.136947| num_tokens: 13,344
Step 00606/00701 | Training loss: 0.647469| lrm: 0.135521| num_tokens: 9,870
Step 00607/00701 | Training loss: 0.846538| lrm: 0.134094| num_tokens: 13,599
Step 00608/00701 | Training loss: 0.881109| lrm: 0.132668| num_tokens: 15,215
Step 00609/00701 | Training loss: 0.474250| lrm: 0.131241| num_tokens: 10,760
Step 00610/00701 | Training loss: 0.538018| lrm: 0.129815| num_tokens: 11,229
Step 00611/00701 | Training loss: 0.508244| lrm: 0.128388| num_tokens: 12,494
Step 00612/00701 | Training loss: 0.329614| lrm: 0.126961| num_tokens: 10,673
Step 00613/00701 | Training loss: 0.694182| lrm: 0.125535| num_tokens: 11,396
Step 00614/00701 | Training loss: 0.694941| lrm: 0.124108| num_tokens: 8,759
Step 00615/00701 | Training loss: 0.746936| lrm: 0.122682| num_tokens: 12,240
Step 00616/00701 | Training loss: 0.766334| lrm: 0.121255| num_tokens: 8,875
Step 00617/00701 | Training loss: 0.516505| lrm: 0.119829| num_tokens: 12,465
Step 00618/00701 | Training loss: 0.464822| lrm: 0.118402| num_tokens: 12,131
Step 00619/00701 | Training loss: 0.353566| lrm: 0.116976| num_tokens: 11,324
Step 00620/00701 | Training loss: 0.624186| lrm: 0.115549| num_tokens: 15,695
Step 00621/00701 | Training loss: 0.876771| lrm: 0.114123| num_tokens: 9,785
Step 00622/00701 | Training loss: 0.598820| lrm: 0.112696| num_tokens: 11,316
Step 00623/00701 | Training loss: 0.493659| lrm: 0.111270| num_tokens: 11,776
Step 00624/00701 | Training loss: 0.697775| lrm: 0.109843| num_tokens: 17,152
Step 00625/00701 | Training loss: 0.176169| lrm: 0.108417| num_tokens: 9,342
Step 00626/00701 | Training loss: 0.563719| lrm: 0.106990| num_tokens: 9,083
Step 00627/00701 | Training loss: 0.367667| lrm: 0.105563| num_tokens: 6,766
Step 00628/00701 | Training loss: 0.656145| lrm: 0.104137| num_tokens: 8,521
Step 00629/00701 | Training loss: 0.483811| lrm: 0.102710| num_tokens: 11,762
Step 00630/00701 | Training loss: 0.968704| lrm: 0.101284| num_tokens: 10,814
Step 00631/00701 | Training loss: 0.545995| lrm: 0.099857| num_tokens: 17,113
Step 00632/00701 | Training loss: 0.413529| lrm: 0.098431| num_tokens: 12,037
Step 00633/00701 | Training loss: 0.469729| lrm: 0.097004| num_tokens: 10,796
Step 00634/00701 | Training loss: 0.699893| lrm: 0.095578| num_tokens: 9,403
Step 00635/00701 | Training loss: 0.844976| lrm: 0.094151| num_tokens: 9,082
Step 00636/00701 | Training loss: 0.633079| lrm: 0.092725| num_tokens: 9,322
Step 00637/00701 | Training loss: 0.682183| lrm: 0.091298| num_tokens: 14,252
Step 00638/00701 | Training loss: 0.441878| lrm: 0.089872| num_tokens: 8,688
Step 00639/00701 | Training loss: 0.517336| lrm: 0.088445| num_tokens: 13,947
Step 00640/00701 | Training loss: 0.679919| lrm: 0.087019| num_tokens: 9,721
Step 00641/00701 | Training loss: 0.656649| lrm: 0.085592| num_tokens: 9,564
Step 00642/00701 | Training loss: 0.495518| lrm: 0.084165| num_tokens: 10,973
Step 00643/00701 | Training loss: 0.569117| lrm: 0.082739| num_tokens: 14,939
Step 00644/00701 | Training loss: 0.681834| lrm: 0.081312| num_tokens: 9,483
Step 00645/00701 | Training loss: 0.606160| lrm: 0.079886| num_tokens: 9,737
Step 00646/00701 | Training loss: 0.556827| lrm: 0.078459| num_tokens: 12,228
Step 00647/00701 | Training loss: 0.925691| lrm: 0.077033| num_tokens: 10,247
Step 00648/00701 | Training loss: 0.484520| lrm: 0.075606| num_tokens: 14,603
Step 00649/00701 | Training loss: 0.549386| lrm: 0.074180| num_tokens: 8,166
Step 00650/00701 | Training loss: 0.747503| lrm: 0.072753| num_tokens: 12,771
Step 00651/00701 | Training loss: 0.815617| lrm: 0.071327| num_tokens: 14,671
Step 00652/00701 | Training loss: 0.647527| lrm: 0.069900| num_tokens: 14,336
Step 00653/00701 | Training loss: 0.631808| lrm: 0.068474| num_tokens: 9,207
Step 00654/00701 | Training loss: 0.580519| lrm: 0.067047| num_tokens: 9,143
Step 00655/00701 | Training loss: 0.613484| lrm: 0.065621| num_tokens: 12,910
Step 00656/00701 | Training loss: 0.501126| lrm: 0.064194| num_tokens: 11,937
Step 00657/00701 | Training loss: 0.699023| lrm: 0.062767| num_tokens: 14,512
Step 00658/00701 | Training loss: 0.477858| lrm: 0.061341| num_tokens: 10,209
Step 00659/00701 | Training loss: 0.511527| lrm: 0.059914| num_tokens: 8,351
Step 00660/00701 | Training loss: 0.771580| lrm: 0.058488| num_tokens: 12,439
Step 00661/00701 | Training loss: 0.845450| lrm: 0.057061| num_tokens: 10,951
Step 00662/00701 | Training loss: 0.476718| lrm: 0.055635| num_tokens: 9,526
Step 00663/00701 | Training loss: 0.492739| lrm: 0.054208| num_tokens: 12,604
Step 00664/00701 | Training loss: 1.256421| lrm: 0.052782| num_tokens: 12,972
Step 00665/00701 | Training loss: 0.219100| lrm: 0.051355| num_tokens: 12,686
Step 00666/00701 | Training loss: 0.823304| lrm: 0.049929| num_tokens: 13,839
Step 00667/00701 | Training loss: 0.340796| lrm: 0.048502| num_tokens: 16,434
Step 00668/00701 | Training loss: 0.340856| lrm: 0.047076| num_tokens: 17,991
Step 00669/00701 | Training loss: 0.424773| lrm: 0.045649| num_tokens: 12,519
Step 00670/00701 | Training loss: 0.868671| lrm: 0.044223| num_tokens: 10,020
Step 00671/00701 | Training loss: 0.720076| lrm: 0.042796| num_tokens: 11,198
Step 00672/00701 | Training loss: 0.464515| lrm: 0.041369| num_tokens: 7,502
Step 00673/00701 | Training loss: 0.910649| lrm: 0.039943| num_tokens: 12,467
Step 00674/00701 | Training loss: 0.408960| lrm: 0.038516| num_tokens: 8,523
Step 00675/00701 | Training loss: 0.343830| lrm: 0.037090| num_tokens: 9,276
Step 00676/00701 | Training loss: 0.733395| lrm: 0.035663| num_tokens: 14,412
Step 00677/00701 | Training loss: 0.696763| lrm: 0.034237| num_tokens: 14,262
Step 00678/00701 | Training loss: 0.512676| lrm: 0.032810| num_tokens: 14,078
Step 00679/00701 | Training loss: 0.865030| lrm: 0.031384| num_tokens: 5,175
Step 00680/00701 | Training loss: 0.413276| lrm: 0.029957| num_tokens: 14,903
Step 00681/00701 | Training loss: 0.414626| lrm: 0.028531| num_tokens: 12,310
Step 00682/00701 | Training loss: 0.599130| lrm: 0.027104| num_tokens: 13,041
Step 00683/00701 | Training loss: 0.345973| lrm: 0.025678| num_tokens: 10,871
Step 00684/00701 | Training loss: 0.759685| lrm: 0.024251| num_tokens: 15,082
Step 00685/00701 | Training loss: 0.660119| lrm: 0.022825| num_tokens: 11,492
Step 00686/00701 | Training loss: 0.735941| lrm: 0.021398| num_tokens: 12,716
Step 00687/00701 | Training loss: 0.394976| lrm: 0.019971| num_tokens: 11,045
Step 00688/00701 | Training loss: 0.467817| lrm: 0.018545| num_tokens: 10,428
Step 00689/00701 | Training loss: 0.617303| lrm: 0.017118| num_tokens: 7,962
Step 00690/00701 | Training loss: 0.845365| lrm: 0.015692| num_tokens: 11,570
Step 00691/00701 | Training loss: 0.490753| lrm: 0.014265| num_tokens: 12,984
Step 00692/00701 | Training loss: 0.632518| lrm: 0.012839| num_tokens: 12,395
Step 00693/00701 | Training loss: 1.126097| lrm: 0.011412| num_tokens: 13,045
Step 00694/00701 | Training loss: 0.770635| lrm: 0.009986| num_tokens: 7,264
Step 00695/00701 | Training loss: 0.340752| lrm: 0.008559| num_tokens: 9,571
Step 00696/00701 | Training loss: 0.582061| lrm: 0.007133| num_tokens: 14,347
Step 00697/00701 | Training loss: 0.515206| lrm: 0.005706| num_tokens: 12,443
Step 00698/00701 | Training loss: 0.459231| lrm: 0.004280| num_tokens: 7,595
Step 00699/00701 | Training loss: 0.388668| lrm: 0.002853| num_tokens: 13,055
Step 00700 | Validation loss: 0.800208
Final: 420/1024 (41.02%)
Final: 415/570 (72.81%)
Step 00700 | mmlu_acc: 0.410156, arc_easy_acc: 0.728070
✅ Saved model checkpoint to /public_hw/share/cit_ztyu/cz/nanochat/chatsft_checkpoints/d26
